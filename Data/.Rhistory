inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confustion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
C1
A1 <- C1$overall[1]
## do similar steps with the caret package
modelFit <- train(training$diagnosis ~ ., method = "glm", preProcess = "pca",
data = training, trControl = trainControl(preProcOptions = list(thresh = 0.8)))
C2 <- confusionMatrix(testing$diagnosis, predict(modelFit, testing))
C2
A2 <- C2$overall[1]
install.packages(c("manipulate", "Rcpp"))
install.packages("ISLR")
library(ISLR);library(ggplot2);library(caret)
data(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[inTrain,]
dim(training);dim(testing)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training);dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],y=traing$wage,plot="pairs")
featurePlot(x=training[,c("age","education","jobClass")],y=traing$wage,plot="pairs")
featurePlot(x=training[,c("age","education","jobclass")],
y=traing$wage,plot="pairs")
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,color=jobclass,data=training)
qq<-qplot(age,wage,color=education,data=training)
qq+geom_smooth((method="lm",formula=y~x)
qq<-qplot(age,wage,color=education,data=training)
qq+geom_smooth(method="lm",formula=y~x)
cutWage<-cut2(training$wage,g=3)
table(cutWage)
cutWage<-cut2(training$wage,g=3)
library(Hmis)
library(Hmisc)
cutWage<-cut2(training$wage,g=3)
table(cutWage)
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
pl
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
library(Hmisc)
grid.arrange(p1,p2,ncol=2)
library(grid)
library(gridExtra)
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
library(ISLR);library(ggplot2);library(caret)
data(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training);dim(testing)
#Ploting training sets
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,color=jobclass,data=training)
qq<-qplot(age,wage,color=education,data=training)
qq+geom_smooth(method="lm",formula=y~x)
#cut2,making factors(Hmisc package)
library(Hmisc)
cutWage<-cut2(training$wage,g=3) # g means 'group'
table(cutWage)
#boxplot
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
pl
#box plot with pts overlayed
library(grid)
library(gridExtra)
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
library(ISLR);library(ggplot2);library(caret)
data(Wage)
summary(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
dim(training);dim(testing)
#Ploting training sets
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,color=jobclass,data=training)
qq<-qplot(age,wage,color=education,data=training)
qq+geom_smooth(method="lm",formula=y~x)
#cut2,making factors(Hmisc package)
library(Hmisc)
cutWage<-cut2(training$wage,g=3) # g means 'group'
table(cutWage)
#boxplot
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
pl
#box plot with pts overlayed
#library(grid)
#library(gridExtra)
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
#Exercise from url<http://rstudio.github.io/shiny/tutorial/#sliders>
library(shiny)
runExample("05_sliders")
library(shiny)
# Define UI for slider demo application
shinyUI(pageWithSidebar(
#  Application title
headerPanel("Sliders"),
# Sidebar with sliders that demonstrate various available options
sidebarPanel(
# Simple integer interval
sliderInput("integer", "Integer:",
min=0, max=1000, value=500),
# Decimal interval with step value
sliderInput("decimal", "Decimal:",
min = 0, max = 1, value = 0.5, step= 0.1),
# Specification of range within an interval
sliderInput("range", "Range:",
min = 1, max = 1000, value = c(200,500)),
# Provide a custom currency format for value display, with basic animation
sliderInput("format", "Custom Format:",
min = 0, max = 10000, value = 0, step = 2500,
format="$#,##0", locale="us", animate=TRUE),
# Animation with custom interval (in ms) to control speed, plus looping
sliderInput("animation", "Looping Animation:", 1, 2000, 1, step = 10,
animate=animationOptions(interval=300, loop=T))
),
# Show a table summarizing the values entered
mainPanel(
tableOutput("values")
)
))
install.packages("shiny")
#Exercise from url<http://rstudio.github.io/shiny/tutorial/#sliders>
library(shiny)
runExample("05_sliders")
shinyUI(bootstrapPage(
selectInput(inputId = "n_breaks",
label = "Number of bins in histogram (approximate):",
choices = c(10, 20, 35, 50),
selected = 20),
checkboxInput(inputId = "individual_obs",
label = strong("Show individual observations"),
value = FALSE),
checkboxInput(inputId = "density",
label = strong("Show density estimate"),
value = FALSE),
plotOutput(outputId = "main_plot", height = "300px"),
# Display this only if the density is shown
conditionalPanel(condition = "input.density == true",
sliderInput(inputId = "bw_adjust",
label = "Bandwidth adjustment:",
min = 0.2, max = 2, value = 1, step = 0.2)
)
))
#Shinyui.r
shinyUI(bootstrapPage(
selectInput(inputId = "n_breaks",
label = "Number of bins in histogram (approximate):",
choices = c(10, 20, 35, 50),
selected = 20),
checkboxInput(inputId = "individual_obs",
label = strong("Show individual observations"),
value = FALSE),
checkboxInput(inputId = "density",
label = strong("Show density estimate"),
value = FALSE),
plotOutput(outputId = "main_plot", height = "300px"),
# Display this only if the density is shown
conditionalPanel(condition = "input.density == true",
sliderInput(inputId = "bw_adjust",
label = "Bandwidth adjustment:",
min = 0.2, max = 2, value = 1, step = 0.2)
)
))
#serverR
shinyUI(bootstrapPage(
selectInput(inputId = "n_breaks",
label = "Number of bins in histogram (approximate):",
choices = c(10, 20, 35, 50),
selected = 20),
checkboxInput(inputId = "individual_obs",
label = strong("Show individual observations"),
value = FALSE),
checkboxInput(inputId = "density",
label = strong("Show density estimate"),
value = FALSE),
plotOutput(outputId = "main_plot", height = "300px"),
# Display this only if the density is shown
conditionalPanel(condition = "input.density == true",
sliderInput(inputId = "bw_adjust",
label = "Bandwidth adjustment:",
min = 0.2, max = 2, value = 1, step = 0.2)
)
))
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
library(Hmisc)
cutWage<-cut2(training$wage,g=3) # g means 'group'
table(cutWage)
#boxplot
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
pl
library(ISLR);library(ggplot2);library(caret)
data(Wage)
inTrain<-createDataPartition(y=Wage$wage,p=0.7,list=FALSE)
training<-Wage[inTrain,]
testing<-Wage[-inTrain,]
featurePlot(x=training[,c("age","education","jobclass")],y=training$wage,plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,color=jobclass,data=training)
qq<-qplot(age,wage,color=education,data=training)
qq+geom_smooth(method="lm",formula=y~x)
#cut2,making factors(Hmisc package)
library(Hmisc)
cutWage<-cut2(training$wage,g=3) # g means 'group'
table(cutWage)
#boxplot
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
pl
library(gridExtra)
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
library(ggplot2)
library(grid)
library(gridExtra)
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
library(ggplot2)
library(grid)
library(gridExtra)
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
install.packages("vegan3d")
library(ggplot2) #for plotting
library(reshape2) #for data reshaping
library(vegan3d) #for the data
data(varechem)
vMelt<-melt(varechem, measure.vars=c("Humdepth", "Baresoil"))
qplot(value, data=vMelt, fill=variable)+facet_wrap( facets=~variable, scale="free_x")
library(gridExtra)
#make two separate ggplot2 objects
humDist<-qplot(Humdepth, data=varechem, fill=I("red"))
bareDist<-qplot(Baresoil, data=varechem, fill=I("blue"))
#Now use grid.arrange to put them all into one figure.
#Note the use of ncol to specify two columns.  Things are nicely flexible here.
grid.arrange(humDist, bareDist, ncol=2)
library(gridExtra)
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("jitter"))
grid.arrange(p1,p2,ncol=2)
corPlot<-qplot(Humdepth, Baresoil, data=varechem, size=I(3))+stat_smooth(method="lm")
blankPanel<-grid.rect(gp=gpar(col="white"))
grid.arrange(humDist, blankPanel, corPlot, bareDist, ncol=2)
a<-anova(lm(Baresoil ~ Humdepth, data=varechem))
grid.table(round(a, digits=3))
a<-anova(lm(Baresoil ~ Humdepth, data=varechem))
grid.arrange(humDist, blankPanel, corPlot, bareDist, ncol=2)
tableGrob(round(a, digits=3))
grid.table(round(a, digits=3))
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot",jitter"))
grid.arrange(p1,p2,ncol=2)
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
arrangeGrob(p1,p2)
pl<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot"))
pl
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(p1,p2,ncol=2)
p1
p2<-qplot(cutWage,age,data=training,fill=cutWage,geom=c("boxplot","jitter"))
grid.arrange(pl,p2,ncol=2)
T1<-table(cutWage,training$jobclass)
T1
prop.table(T1,1)
qplot(wage,colour=education,data=training,geom="density")
library(caret);library(kernlab);data(spam)
inTrain<-createDataPartition(y=spam$type,p=0.75,list=FALSE)
training<-spam[inTrain,]
testing<-spam[-inTrain,]
hist(training$CapitalAve,main="",xlab="ave,capital run length")
hist(training$CapitalAve,main="",xlab="ave.apital run length")
hist(training$capitalAve,main="",xlab="ave.capital run lenght")
mean(training$capitalAve)
sd(training$capitalAve)
tainCapAve<-training$capitalAve
trainCapAve5<-(trainCapAve-mean(trainCapAve))/sd(trainCapAve)
trainCapAve<-training$capitalAve
trainCapAve5<-(trainCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAve5)
sd(trainCapAve5)
testCapAve<-testing$capitalAve
testCapAve5<-(testCapAve-mean(trainCapAve))/sd(trainCapAve))
testCapAve5<-(testCapAve-mean(trainCapAve))/sd(trainCapAve)
mean(testCapAve)
mean(testCapAve5)
sd(testCapAve5)
preObj<-preProcess(trraining[,-58],method=c("center","scale"))
preObj<-preProcess(training[,-58],method=c("center","scale"))
trainCapAve5<-predict(preObj,training[,-58])$capitalAve
mean(trainCapAve5)
sd(trainCapAve5)
data(BloodBrain)
View(bbbDescr)
preProc <- preProcess(bbbDescr[1:100,-3])
training <- predict(preProc, bbbDescr[1:100,-3])
test <- predict(preProc, bbbDescr[101:208,-3])
mean(training)
View(training)
mean(training$tpsa)
sd(training$tpsa)
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData <- data.frame(diagnosis, predictors)
inTrain <- createDataPartition(adData$diagnosis, p=3/4)[[1]]
training <- adData[inTrain, ]
testing <- adData[-inTrain, ]
dim(adData) # 333 131
# head(adData)
set.seed(62433)
fitRf <- train(diagnosis ~ ., data=training, method="rf")
fitGBM <- train(diagnosis ~ ., data=training, method="gbm")
fitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRf <- predict(fitRf, testing)
predGBM <- predict(fitGBM, testing)
predLDA <- predict(fitLDA, testing)
pred <- data.frame(predRf, predGBM, predLDA, diagnosis=testing$diagnosis)
# Stack the predictions together using random forests ("rf")
fit <- train(diagnosis ~., data=pred, method="rf")
predFit <- predict(fit, testing)
c1 <- confusionMatrix(predRf, testing$diagnosis)$overall[1]
c2 <- confusionMatrix(predGBM, testing$diagnosis)$overall[1]
c3 <- confusionMatrix(predLDA, testing$diagnosis)$overall[1]
c4 <- confusionMatrix(predFit, testing$diagnosis)$overall[1]
print(paste(c1, c2, c3, c4))
# Stacked Accuracy: 0.79 is better than random forests and lda
# and the same as boosting.
set.seed(3523)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain <- createDataPartition(concrete$CompressiveStrength,
p=3/4)[[1]]
training <- concrete[inTrain, ]
testing <- concrete[-inTrain, ]
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
set.seed(3523)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain <- createDataPartition(concrete$CompressiveStrength,
p=3/4)[[1]]
training <- concrete[inTrain, ]
testing <- concrete[-inTrain, ]
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
install.packages("elasticnet")
set.seed(3523)
library(AppliedPredictiveModeling)
library(elasticnet)
data(concrete)
inTrain <- createDataPartition(concrete$CompressiveStrength,
p=3/4)[[1]]
training <- concrete[inTrain, ]
testing <- concrete[-inTrain, ]
set.seed(233)
fit <- train(CompressiveStrength ~ ., data=training, method="lasso")
fit
plot.enet(fit$finalModel, xvar="penalty", use.color=T) # Cement
library(lubridate)  # For year() function below
library(forecast)
dat <- read.csv("./data/gaData.csv")
training <- dat[year(dat$date) < 2012, ]
testing <- dat[(year(dat$date)) > 2011, ]
tstrain <- ts(training$visitsTumblr)
fit <- bats(tstrain)
fit
pred <- forecast(fit, level=95, h=dim(testing)[1])
names(data.frame(pred))
predComb <- cbind(testing, data.frame(pred))
names(testing)
names(predComb)
predComb$in95 <- (predComb$Lo.95 < predComb$visitsTumblr) &
(predComb$visitsTumblr < predComb$Hi.95)
# How many of the testing points is the true value within the
# 95% prediction interval bounds?
prop.table(table(predComb$in95))[2] # 0.9617021
install.packages("forecast")
library(lubridate)  # For year() function below
library(forecast)
dat <- read.csv("./data/gaData.csv")
training <- dat[year(dat$date) < 2012, ]
testing <- dat[(year(dat$date)) > 2011, ]
tstrain <- ts(training$visitsTumblr)
fit <- bats(tstrain)
fit
pred <- forecast(fit, level=95, h=dim(testing)[1])
names(data.frame(pred))
predComb <- cbind(testing, data.frame(pred))
names(testing)
names(predComb)
predComb$in95 <- (predComb$Lo.95 < predComb$visitsTumblr) &
(predComb$visitsTumblr < predComb$Hi.95)
# How many of the testing points is the true value within the
# 95% prediction interval bounds?
prop.table(table(predComb$in95))[2] # 0.9617021
getwd()
dat <- read.csv("./data/gaData.csv")
library(lubridate)  # For year() function below
library(forecast)
getwd()
dat <- read.csv("./data/gaData.csv")
training <- dat[year(dat$date) < 2012, ]
testing <- dat[(year(dat$date)) > 2011, ]
tstrain <- ts(training$visitsTumblr)
fit <- bats(tstrain)
fit
pred <- forecast(fit, level=95, h=dim(testing)[1])
names(data.frame(pred))
predComb <- cbind(testing, data.frame(pred))
names(testing)
names(predComb)
predComb$in95 <- (predComb$Lo.95 < predComb$visitsTumblr) &
(predComb$visitsTumblr < predComb$Hi.95)
# How many of the testing points is the true value within the
# 95% prediction interval bounds?
prop.table(table(predComb$in95))[2] # 0.9617021
set.seed(3523)
library(AppliedPredictiveModeling)
library(e1071)
data(concrete)
inTrain <- createDataPartition(concrete$CompressiveStrength, p=3/4)[[1]]
training <- concrete[inTrain, ]
testing <- concrete[-inTrain, ]
set.seed(325)
fit <- svm(CompressiveStrength ~., data=training)
# OR another way
# fit <- train(CompressiveStrength ~. data=training, method="svmRadial")
pred <- predict(fit, testing)
acc <- accuracy(pred, testing$CompressiveStrength)
acc
acc[2] # RMSE 6.715009
getwd()
setwd("~./Data")
setwd("C:/Users/User/Desktop/Rdirectory/Data")
training= read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
testing = read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
dim(training)
dim(testing)
getwd()
---
title: "Prediction on Exercise Behaviour"
author: "Kyaw Thu"
date: "Tuesday, September 15, 2015"
output: html_document
---
```{r, echo=FALSE}
```
#Introduction#
Jawbone Up,Nike FuelBand, andFitbit are popular devices to measure the personal activities data inexpensively and conveniently.One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.
In this project, targeted goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict how they do their exercise. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.
##Data##
Training and test data are downloaded from the follwing link and located in Data folder:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv> ;
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>
```{r,echo=FALSE}
setwd("C:/Users/User/Desktop/Rdirectory/Data")
```
#Read Data#
```{r}
training= read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
testing = read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
dim(training)
dim(testing)
```
getwd()
training= read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
testing = read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
dim(training)
dim(testing)
training= read.csv("pml-training.csv", na.strings=c("", "NA", "NULL"))
testing = read.csv("pml-testing.csv", na.strings=c("", "NA", "NULL"))
dim(training)
dim(testing)
data <- read.csv("pml-training.csv")
install.packages("doParallel")
